{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "from genericpath import exists\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import T_co\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AdamW\n",
    "from transformers import Adafactor\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from ClassesAndFunctions import *\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Loading data...\n",
      "Dataset initialized...\n",
      "Training data:\t\t5693\t 90%\n",
      "Validation data:\t316\t 5%\n",
      "Test data:\t\t317\t 5%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "num_epoch = 1\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ufal/byt5-small-multilexnorm2021-da\")\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "print(\"Optimizer: Adam\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# If dataset is already generated as a pickle it can be used setting boolean to False\n",
    "reloadData = True\n",
    "if reloadData:\n",
    "\tdataset = MultiLexDataset(path_to_files=[\"final_nst.txt\", \"final_audiobooks.txt\"], only_include_corrections=False, short_data=True)\n",
    "\twith open('dataset.pickle', 'wb') as f:\n",
    "\t\tpickle.dump(dataset, f)\n",
    "else:\n",
    "\twith open('dataset.pickle', 'rb') as f:\n",
    "\t\tdataset = pickle.load(f)\n",
    "\n",
    "# Use with a datalodaer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ufal/byt5-small-multilexnorm2021-da\")\n",
    "trainloader = DataLoader(dataset.train, batch_size=batch_size, collate_fn=CollateFunctor(tokenizer))\n",
    "validationloader = DataLoader(dataset.validation, batch_size=1, collate_fn=CollateFunctor(tokenizer))\n",
    "testloader = DataLoader(dataset.test, batch_size=1, collate_fn=CollateFunctor(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "num_training_steps = int(num_epoch * np.ceil(len(dataset.train)/batch_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initiate training\")\n",
    "progress_bar = tqdm(range(num_training_steps), position=0)\n",
    "model.train()\n",
    "\n",
    "eval_every = round(num_training_steps*0.01) # 1%\n",
    "first = True\n",
    "steps_training_plot = []\n",
    "steps_validation_plot = []\n",
    "trainingLossArr = [] \n",
    "validationLossArr = []\n",
    "current_training_batch = 0\n",
    "\n",
    "with open(\"lossData.txt\", \"a\") as file_object:\n",
    "\t\t\t\tfile_object.write('Samples in per epoch:\\t' + str(np.ceil(len(dataset.train)/batch_size)) + '\\t Nb epochs:\\t' + str(num_epoch) + '\\n')\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\tfor batch_idx, batch in enumerate(trainloader):\n",
    "\t\t#if batch_idx < 106866:  # Only for a specific continuation of a stopped session\n",
    "\t\t#\tcontinue\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tcurrent_training_batch += 1\n",
    "\t\tsteps_training_plot.append(current_training_batch)\n",
    "\t\tmodel.train()\n",
    "      \n",
    "      \t# Move batch to GPU\n",
    "\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      #Make prediction with the model\n",
    "\t\toutput = model(input_ids=batch[\"input_ids\"],attention_mask=batch[\"attention_mask\"],labels=batch[\"labels\"],decoder_attention_mask=batch[\"decoder_attention_mask\"])\n",
    "\n",
    "      # Zero_grad, backwards and optimizer step\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss = output.loss\n",
    "\t\ttrainingLossArr.append(loss.item())\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\n",
    "      # Do validation\n",
    "\t\tif batch_idx % eval_every == 0 or batch_idx == len(trainloader):\n",
    "\t\t\tprogress_bar_eval = tqdm(total=len(dataset.validation), position=1)\n",
    "\t\t\tsteps_validation_plot.append(current_training_batch)\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tvalidationLoss2Mean = 0\n",
    "\t\t\tfor num, batch_validation in enumerate(validationloader):\n",
    "\t\t\t\tbatch_validation = {k: v.to(device) for k, v in batch_validation.items()}\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\toutput = model(**batch_validation)\n",
    "\n",
    "\t\t\t\tvalidationLoss2Mean += output.loss.item()\n",
    "\n",
    "\t\t\t\tprogress_bar_eval.update(1)\n",
    "\n",
    "\t\t\tvalidationLoss2Mean /= len(validationloader)\n",
    "\n",
    "\t\t\tmodelName = \"models/model_\" + str(current_training_batch)\n",
    "\t\t\t\n",
    "\t\t\tif not os.path.exists(\"models\"):\n",
    "\t\t\t\tos.mkdir(\"models\")\n",
    "\t\t\t\n",
    "\t\t\tif batch_idx >= 7*10**4 and batch_idx % eval_every*3 == 0:\n",
    "\t\t\t\ttorch.save(model.state_dict(),modelName)\n",
    "\n",
    "\t\t\tEvaluatedMetricsTest = testsetAgainstNLPMetrics(dataset, tokenizer, model, device)\n",
    "\n",
    "\t\t\twith open(\"lossData.txt\", \"a\") as file_object:\n",
    "\t\t\t\tfile_object.write(str(datetime.now()) + '\\t' + str(current_training_batch) + '\\t' + str(np.mean(trainingLossArr)) + '\\t' + \n",
    "\t\t\t\tstr(validationLoss2Mean) + '\\t' + str(EvaluatedMetricsTest.errorCalc.errorMeanMetrics[\"wer\"]) + '\\t' + \n",
    "\t\t\t\tstr(EvaluatedMetricsTest.errorCalc.errorMeanMetrics[\"bleu\"]) + '\\t' + str(EvaluatedMetricsTest.errorCalc.errorMeanMetrics[\"gleu\"]) + '\\n')\n",
    "\t\t\t\n",
    "\t\t\ttrainingLossArr = []\n",
    "\n",
    "\t\t\n",
    "\t\tprogress_bar.update(1)\n",
    "  \n",
    "\t# kill batch\n",
    "\tdel batch"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6355558afb2c03976442ebe79a14d2dbb2da56920c044596abed84e99b3f7f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
